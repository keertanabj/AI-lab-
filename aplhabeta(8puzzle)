import math
import copy

# Goal state
GOAL = [1, 2, 3,
        4, 5, 6,
        7, 8, 0]  # 0 = blank space

def print_board(state, show_positions=False):
    for i in range(0, 9, 3):
        row = []
        for j in range(3):
            idx = i + j
            if show_positions and state[idx] == 0:
                row.append("_")
            elif show_positions:
                row.append(str(state[idx]))
            else:
                row.append("_" if state[idx] == 0 else str(state[idx]))
        print(" | ".join(row))
    print()

def get_neighbors(state):
    """Generate possible moves by sliding the blank (0)."""
    neighbors = []
    i = state.index(0)
    x, y = divmod(i, 3)
    moves = [(-1,0),(1,0),(0,-1),(0,1)]  # up, down, left, right
    for dx, dy in moves:
        nx, ny = x + dx, y + dy
        if 0 <= nx < 3 and 0 <= ny < 3:
            new_state = state[:]
            j = nx * 3 + ny
            new_state[i], new_state[j] = new_state[j], new_state[i]
            neighbors.append(new_state)
    return neighbors

def heuristic(state):
    """Manhattan distance heuristic."""
    dist = 0
    for i, val in enumerate(state):
        if val == 0:
            continue
        goal_x, goal_y = divmod(val - 1, 3)
        cur_x, cur_y = divmod(i, 3)
        dist += abs(goal_x - cur_x) + abs(goal_y - cur_y)
    return dist

def is_goal(state):
    """Check if state is the goal."""
    return state == GOAL

def alpha_beta(state, depth, alpha, beta, maximizing_player, visited):
    """Alphaâ€“Beta search adapted for a single-agent puzzle."""
    if is_goal(state):
        return 1000 - depth  # reward faster goal
    if depth == 0:
        return -heuristic(state)

    visited.add(tuple(state))
    if maximizing_player:
        max_eval = -math.inf
        for neighbor in get_neighbors(state):
            if tuple(neighbor) in visited:
                continue
            eval = alpha_beta(neighbor, depth - 1, alpha, beta, False, visited)
            max_eval = max(max_eval, eval)
            alpha = max(alpha, eval)
            if beta <= alpha:
                break
        visited.remove(tuple(state))
        return max_eval
    else:
        min_eval = math.inf
        for neighbor in get_neighbors(state):
            if tuple(neighbor) in visited:
                continue
            eval = alpha_beta(neighbor, depth - 1, alpha, beta, True, visited)
            min_eval = min(min_eval, eval)
            beta = min(beta, eval)
            if beta <= alpha:
                break
        visited.remove(tuple(state))
        return min_eval

def best_move(state, depth=5):
    """Chooses the next move using alphaâ€“beta pruning."""
    best_val = -math.inf
    best_state = None
    for neighbor in get_neighbors(state):
        val = alpha_beta(neighbor, depth, -math.inf, math.inf, False, set())
        if val > best_val:
            best_val = val
            best_state = neighbor
    return best_state

def play_puzzle():
    print("Position guide (0 = blank):")
    print("0 | 1 | 2\n3 | 4 | 5\n6 | 7 | 8\n")

    # ðŸ”¹ Start state that reaches goal in 3 moves
    start = [1, 2, 3,
             4, 5, 6,
             0, 7, 8]

    print("Initial State:")
    print_board(start)

    current = start
    steps = 0

    while not is_goal(current) and steps < 20:
        print(f"Step {steps + 1}:")
        next_state = best_move(current, depth=6)
        if not next_state:
            print("No better move found (local minimum).")
            break
        print_board(next_state)
        current = next_state
        steps += 1

    if is_goal(current):
        print(f"ðŸŽ¯ Goal reached in {steps} steps!")
    else:
        print("Search stopped (try increasing depth).")

if __name__ == "__main__":
    play_puzzle()
